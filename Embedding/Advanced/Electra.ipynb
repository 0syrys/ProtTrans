{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Electra.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agemagician/Prot-Transformers/blob/master/Embedding/Advanced/Electra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR33JAEDKPTm",
        "colab_type": "text"
      },
      "source": [
        "<h3> Extracting protein sequences' features using Electra pretrained-model <h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUDW7HTPKPTq",
        "colab_type": "text"
      },
      "source": [
        "<b>1. Load necessry libraries including huggingface transformers<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCL1_YD4KWtj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "677ed1cc-0265-47b4-90b0-bcc3a0e40a39"
      },
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q gdown"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 675kB 4.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 13.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 41.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.2MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbdpbXQXKPTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import ElectraTokenizer, ElectraForPreTraining, ElectraForMaskedLM, ElectraModel\n",
        "import re\n",
        "import gdown\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POCwBLviKPT7",
        "colab_type": "text"
      },
      "source": [
        "<b>2. Set the url location of Electra and the vocabulary file<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VJMeqwZKPT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generatorModelUrl = 'https://drive.google.com/uc?export=download&confirm=BTQ_&id=1vaB80ioD8MNFB3zE_5AD-QJtNy0389jg'\n",
        "discriminatorModelUrl = 'https://drive.google.com/uc?export=download&confirm=BTQ_&id=1xMUwFYs4tgD7qIs7XrrqQ6tKabH7ZyS9'\n",
        "\n",
        "generatorConfigUrl = 'https://drive.google.com/uc?export=download&confirm=BTQ_&id=1SBtS-9_Wy26vZDjXBEos9KuiQc7TChhT'\n",
        "discriminatorConfigUrl = 'https://drive.google.com/uc?export=download&confirm=BTQ_&id=1jZQLHL4TTMK5eoWL-JhihiVRVoUepC_B'\n",
        "\n",
        "vocabUrl = 'https://drive.google.com/uc?export=download&confirm=BTQ_&id=1vuAP1zRvN1c6EHoSQMVC2ivZMTpzYR0P'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkJpMktfKPUB",
        "colab_type": "text"
      },
      "source": [
        "<b>3. Download Electra models and vocabulary files<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g3ReBdWKPUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloadFolderPath = 'models/electra/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PCAbwyAKPUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminatorFolderPath = os.path.join(downloadFolderPath, 'discriminator')\n",
        "generatorFolderPath = os.path.join(downloadFolderPath, 'generator')\n",
        "\n",
        "discriminatorModelFilePath = os.path.join(discriminatorFolderPath, 'pytorch_model.bin')\n",
        "generatorModelFilePath = os.path.join(generatorFolderPath, 'pytorch_model.bin')\n",
        "\n",
        "discriminatorConfigFilePath = os.path.join(discriminatorFolderPath, 'config.json')\n",
        "generatorConfigFilePath = os.path.join(generatorFolderPath, 'config.json')\n",
        "\n",
        "vocabFilePath = os.path.join(downloadFolderPath, 'vocab.txt')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frD2ICtAKPUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(discriminatorFolderPath):\n",
        "    os.makedirs(discriminatorFolderPath)\n",
        "if not os.path.exists(generatorFolderPath):\n",
        "    os.makedirs(generatorFolderPath)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsZvou63KiLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_file(url,filename):\n",
        "  while not os.path.exists(filename):\n",
        "    gdown.download(url,filename, quiet=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-FAoD9oKPUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "2c8d0ab7-bc03-42d4-907b-10643ab43464"
      },
      "source": [
        "if not os.path.exists(generatorModelFilePath):\n",
        "    download_file(generatorModelUrl, generatorModelFilePath)\n",
        "\n",
        "if not os.path.exists(discriminatorModelFilePath):\n",
        "    download_file(discriminatorModelUrl, discriminatorModelFilePath)\n",
        "    \n",
        "if not os.path.exists(generatorConfigFilePath):\n",
        "    download_file(generatorConfigUrl, generatorConfigFilePath)\n",
        "\n",
        "if not os.path.exists(discriminatorConfigFilePath):\n",
        "    download_file(discriminatorConfigUrl, discriminatorConfigFilePath)\n",
        "    \n",
        "if not os.path.exists(vocabFilePath):\n",
        "    download_file(vocabUrl, vocabFilePath)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&confirm=BTQ_&id=1vaB80ioD8MNFB3zE_5AD-QJtNy0389jg\n",
            "To: /content/models/electra/generator/pytorch_model.bin\n",
            "261MB [00:02, 94.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&confirm=BTQ_&id=1xMUwFYs4tgD7qIs7XrrqQ6tKabH7ZyS9\n",
            "To: /content/models/electra/discriminator/pytorch_model.bin\n",
            "1.68GB [00:17, 93.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&confirm=BTQ_&id=1SBtS-9_Wy26vZDjXBEos9KuiQc7TChhT\n",
            "To: /content/models/electra/generator/config.json\n",
            "100%|██████████| 463/463 [00:00<00:00, 840kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&confirm=BTQ_&id=1jZQLHL4TTMK5eoWL-JhihiVRVoUepC_B\n",
            "To: /content/models/electra/discriminator/config.json\n",
            "100%|██████████| 468/468 [00:00<00:00, 669kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&confirm=BTQ_&id=1vuAP1zRvN1c6EHoSQMVC2ivZMTpzYR0P\n",
            "To: /content/models/electra/vocab.txt\n",
            "100%|██████████| 81.0/81.0 [00:00<00:00, 80.1kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmvzQ0SHKPUg",
        "colab_type": "text"
      },
      "source": [
        "<b>4. Load the vocabulary and Electra discriminator and generator Models<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGrGgJ5lKPUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = ElectraTokenizer(vocabFilePath, do_lower_case=False )"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFHSXHjqKPUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator = ElectraForPreTraining.from_pretrained(discriminatorFolderPath)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgrS7pgtKPUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = ElectraForMaskedLM.from_pretrained(generatorFolderPath)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6xt3_aKKPUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "electra = ElectraModel.from_pretrained(discriminatorFolderPath)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z48W0VZEKPUv",
        "colab_type": "text"
      },
      "source": [
        "<b>5. Load the model into the GPU if avilabile and switch to inference mode<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL3RJJjLKPUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-zBlpXPKPU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator = discriminator.to(device)\n",
        "discriminator = discriminator.eval()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__8ClKngKPU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = generator.to(device)\n",
        "generator = generator.eval()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Na0O17DbKPU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "electra = electra.to(device)\n",
        "electra = electra.eval()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JbZ5C31KPVI",
        "colab_type": "text"
      },
      "source": [
        "<b>6. Create or load sequences and map rarely occured amino acids (U,Z,O,B) to (X)<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY3MUQYjKPVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences_Example = [\"A E T C Z A O\",\"S K T Z P\"]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AANzWoJiKPVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences_Example = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences_Example]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SbbPA71KPVQ",
        "colab_type": "text"
      },
      "source": [
        "<b>7. Tokenize, encode sequences and load it into the GPU if possibile<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-Xb32-LKPVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ids = vocab.batch_encode_plus(sequences_Example, add_special_tokens=True, pad_to_max_length=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlVXhGqsKPVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = torch.tensor(ids['input_ids']).to(device)\n",
        "attention_mask = torch.tensor(ids['attention_mask']).to(device)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbspbdr3KPVY",
        "colab_type": "text"
      },
      "source": [
        "<b>8. Extracting sequences' features and load it into the CPU if needed<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swfBpdeAKPVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    discriminator_embedding = discriminator(input_ids=input_ids,attention_mask=attention_mask)[0]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJw3fEo6KPVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator_embedding = discriminator_embedding.cpu().numpy()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywW-xlsoKPVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    generator_embedding = generator(input_ids=input_ids,attention_mask=attention_mask)[0]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9duAtxP3KPVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_embedding = generator_embedding.cpu().numpy()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLHvyx9BKPVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    electra_embedding = electra(input_ids=input_ids,attention_mask=attention_mask)[0]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIGpmuMjKPVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "electra_embedding = electra_embedding.cpu().numpy()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCM4efSLKPVv",
        "colab_type": "text"
      },
      "source": [
        "<b>9. Remove padding ([PAD]) and special tokens ([CLS],[SEP]) that is added by Electra model<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M_qpw9sKPVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = [] \n",
        "for seq_num in range(len(electra_embedding)):\n",
        "    seq_len = (attention_mask[seq_num] == 1).sum()\n",
        "    seq_emd = electra_embedding[seq_num][1:seq_len-1]\n",
        "    features.append(seq_emd)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx63uP-MKPVx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "4921e676-f019-49dc-f014-9c7b3c8ce517"
      },
      "source": [
        "print(features)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[-3.11754458e-02, -1.18080616e-01, -1.51422679e-01, ...,\n",
            "        -8.80782455e-02, -2.03649044e-01,  2.34545898e-02],\n",
            "       [-6.92143589e-02, -7.63380080e-02, -1.78088211e-02, ...,\n",
            "        -4.15132381e-02, -3.08615528e-02, -8.58288854e-02],\n",
            "       [ 3.80904488e-02, -1.71692267e-01, -5.64219430e-02, ...,\n",
            "        -1.18378937e-01, -9.77956504e-02,  2.44725216e-02],\n",
            "       ...,\n",
            "       [ 1.27263516e-01, -1.34989679e-01, -3.06518644e-01, ...,\n",
            "         3.99149172e-02, -4.54527065e-02, -3.57910693e-01],\n",
            "       [-5.05245999e-02, -9.02514085e-02,  6.78477362e-02, ...,\n",
            "        -4.76730466e-02, -9.57428291e-02, -1.68221351e-02],\n",
            "       [ 3.07775717e-02,  7.57525049e-05, -5.32222912e-02, ...,\n",
            "        -1.47995083e-02, -1.57044619e-01, -9.64660496e-02]], dtype=float32), array([[-6.04737513e-02, -1.60797983e-01, -1.63700715e-01, ...,\n",
            "        -7.67330825e-02, -1.51252389e-01, -4.52133343e-02],\n",
            "       [-9.30745900e-02, -5.02012298e-02, -1.62957162e-02, ...,\n",
            "        -2.65192648e-04, -2.70886812e-03, -2.37740427e-02],\n",
            "       [-4.23046909e-02, -1.51860267e-01, -6.50829077e-02, ...,\n",
            "        -1.45550948e-02, -7.37645999e-02, -5.66908680e-02],\n",
            "       [ 4.75764386e-02, -7.08769858e-02, -2.75032073e-01, ...,\n",
            "        -1.02747001e-01, -1.06809154e-01, -2.18676060e-01],\n",
            "       [-4.99716476e-02,  1.03257410e-02, -9.70151871e-02, ...,\n",
            "        -1.18601866e-01,  7.05851475e-03, -1.71629295e-01]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}